{
  "hash": "429ee6a17aa753f7b5ca6c2c28186829",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Prediction + model validation\nsubtitle: Lecture 18\nformat: revealjs\neditor_options:\n  chunk_output_type: console\nexecute:\n  warning: false\n  error: false\n---\n\n## Setup {.smaller}\n\n::: {#4457c5dd .cell message='false' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\", rc={\"figure.figsize\": (8, 6), \"axes.labelsize\": 16, \"xtick.labelsize\": 14, \"ytick.labelsize\": 14})\n```\n:::\n\n\n# Model selection\n\n## Data: Candy Rankings {.smaller}\n\n::: {#8fdbd026 .cell execution_count=2}\n``` {.python .cell-code}\ncandy_rankings = pd.read_csv(\"data/candy_rankings.csv\")\n\ncandy_rankings.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 85 entries, 0 to 84\nData columns (total 13 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   competitorname    85 non-null     object \n 1   chocolate         85 non-null     bool   \n 2   fruity            85 non-null     bool   \n 3   caramel           85 non-null     bool   \n 4   peanutyalmondy    85 non-null     bool   \n 5   nougat            85 non-null     bool   \n 6   crispedricewafer  85 non-null     bool   \n 7   hard              85 non-null     bool   \n 8   bar               85 non-null     bool   \n 9   pluribus          85 non-null     bool   \n 10  sugarpercent      85 non-null     float64\n 11  pricepercent      85 non-null     float64\n 12  winpercent        85 non-null     float64\ndtypes: bool(9), float64(3), object(1)\nmemory usage: 3.5+ KB\n```\n:::\n:::\n\n\n## Full model {.smaller}\n\n::: question\nWhat percent of the variability in win percentages is explained by the model?\n:::\n\n::: {#dcf490a5 .cell execution_count=3}\n``` {.python .cell-code}\nfull_model = smf.ols('winpercent ~ chocolate + fruity + caramel + peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent', data=candy_rankings).fit()\nprint(f'R-squared: {full_model.rsquared.round(3)}')\nprint(f'Adjusted R-squared: {full_model.rsquared_adj.round(3)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR-squared: 0.54\nAdjusted R-squared: 0.471\n```\n:::\n:::\n\n\n## Akaike Information Criterion {.smaller}\n\n$$ AIC = -2log(L) + 2k $$\n\n::: incremental\n-   $L$: likelihood of the model\n    -   Likelihood of seeing these data given the estimated model parameters\n    -   Won't go into calculating it in this course (but you will in future courses)\n-   Used for model selection, lower the better\n    -   Value is not informative on its own\n-   Applies a penalty for number of parameters in the model, $k$\n    -   Different penalty than adjusted $R^2$ but similar idea\n:::\n\n::: fragment\n\n::: {#aic-full-model .cell execution_count=4}\n``` {.python .cell-code}\nprint(f'AIC: {full_model.aic}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAIC: 655.2701107463729\n```\n:::\n:::\n\n\n:::\n\n## Model selection -- a little faster {.smaller}\n\n::: {#ef64be37 .cell execution_count=5}\n``` {.python .cell-code}\nselected_model = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=candy_rankings).fit()\n```\n:::\n\n\n::: {#bddd3339 .cell execution_count=6}\n``` {.python .cell-code}\nprint(selected_model.summary2())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     Results: Ordinary least squares\n=========================================================================\nModel:                  OLS                Adj. R-squared:       0.492   \nDependent Variable:     winpercent         AIC:                  647.5113\nDate:                   2024-07-30 14:08   BIC:                  664.6099\nNo. Observations:       85                 Log-Likelihood:       -316.76 \nDf Model:               6                  F-statistic:          14.54   \nDf Residuals:           78                 Prob (F-statistic):   4.62e-11\nR-squared:              0.528              Scale:                110.07  \n-------------------------------------------------------------------------\n                          Coef.  Std.Err.    t    P>|t|   [0.025   0.975]\n-------------------------------------------------------------------------\nIntercept                32.9406   3.5175  9.3647 0.0000  25.9377 39.9434\nchocolate[T.True]        19.1470   3.5870  5.3379 0.0000  12.0059 26.2882\nfruity[T.True]            8.8815   3.5606  2.4944 0.0147   1.7929 15.9701\npeanutyalmondy[T.True]    9.4829   3.4464  2.7516 0.0074   2.6217 16.3440\ncrispedricewafer[T.True]  8.3851   4.4843  1.8699 0.0653  -0.5425 17.3127\nhard[T.True]             -5.6693   3.2889 -1.7238 0.0887 -12.2170  0.8784\nsugarpercent              7.9789   4.1289  1.9325 0.0569  -0.2410 16.1989\n-------------------------------------------------------------------------\nOmnibus:                 0.545           Durbin-Watson:             1.735\nProb(Omnibus):           0.761           Jarque-Bera (JB):          0.682\nSkew:                    -0.093          Prob(JB):                  0.711\nKurtosis:                2.602           Condition No.:             6    \n=========================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n```\n:::\n:::\n\n\n## Selected variables {.smaller}\n\n| variable         | selected |\n|------------------|:--------:|\n| chocolate        |    x     |\n| fruity           |    x     |\n| caramel          |          |\n| peanutyalmondy   |    x     |\n| nougat           |          |\n| crispedricewafer |    x     |\n| hard             |    x     |\n| bar              |          |\n| pluribus         |          |\n| sugarpercent     |    x     |\n| pricepercent     |          |\n\n------------------------------------------------------------------------\n\n## Coefficient interpretation {.smaller}\n\n::: question\nInterpret the slopes of `chocolate` and `sugarpercent` in context of the data.\n:::\n\n::: {#768b8eb3 .cell execution_count=7}\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept                   32.940573\nchocolate[T.True]           19.147029\nfruity[T.True]               8.881496\npeanutyalmondy[T.True]       9.482858\ncrispedricewafer[T.True]     8.385138\nhard[T.True]                -5.669297\nsugarpercent                 7.978930\ndtype: float64\n```\n:::\n:::\n\n\n## AIC {.smaller}\n\nAs expected, the selected model has a smaller AIC than the full model.\nIn fact, the selected model has the minimum AIC of all possible main effects models.\n\n::: {#30e83262 .cell execution_count=8}\n``` {.python .cell-code}\nprint(f'AIC: {full_model.aic}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAIC: 655.2701107463729\n```\n:::\n:::\n\n\n::: {#7657484f .cell execution_count=9}\n``` {.python .cell-code}\nprint(f'AIC: {selected_model.aic}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAIC: 647.5113366053722\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Parismony {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n::: question\nLook at the variables in the full and the selected model.\nCan you guess why some of them may have been dropped?\nRemember: We like parsimonious models.\n:::\n:::\n\n::: {.column width=\"50%\"}\n| variable         | selected |\n|------------------|:--------:|\n| chocolate        |    x     |\n| fruity           |    x     |\n| caramel          |          |\n| peanutyalmondy   |    x     |\n| nougat           |          |\n| crispedricewafer |    x     |\n| hard             |    x     |\n| bar              |          |\n| pluribus         |          |\n| sugarpercent     |    x     |\n| pricepercent     |          |\n:::\n:::\n\n# Prediction\n\n## New observation {.smaller}\n\nTo make a prediction for a new observation we need to create a data frame with that observation.\n\n::: question\nSuppose we want to make a prediction for a candy that contains chocolate, isn't fruity, has no peanuts or almonds, has a wafer, isn't hard, and has a sugar content in the 20th percentile.\n\n<br>\n\nThe following will result in an incorrect prediction.\nWhy?\nHow would you correct it?\n:::\n\n::: {#689e85eb .cell execution_count=10}\n``` {.python .cell-code}\nnew_candy = pd.DataFrame({'chocolate': [1], 'fruity': [0], 'peanutyalmondy': [0], 'crispedricewafer': [1], 'hard': [0], 'sugarpercent': [20]})\n```\n:::\n\n\n## New observation, corrected {.smaller}\n\n::: {#70866486 .cell execution_count=11}\n``` {.python .cell-code}\nnew_candy = pd.DataFrame({'chocolate': [1], 'fruity': [0], 'peanutyalmondy': [0], 'crispedricewafer': [1], 'hard': [0], 'sugarpercent': [0.20]})\nnew_candy\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chocolate</th>\n      <th>fruity</th>\n      <th>peanutyalmondy</th>\n      <th>crispedricewafer</th>\n      <th>hard</th>\n      <th>sugarpercent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Prediction {.smaller}\n\n::: {#449279a6 .cell execution_count=12}\n``` {.python .cell-code}\nprediction = selected_model.predict(new_candy)\nprint(f'Prediction: {prediction}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrediction: 0    62.068526\ndtype: float64\n```\n:::\n:::\n\n\n## Uncertainty around prediction {.smaller}\n\n::: fragment\n-   Confidence interval around $\\hat{y}$ for new data (average win percentage for candy types with the given characteristics):\n\n::: {#2165aedc .cell execution_count=13}\n``` {.python .cell-code}\nconfidence_interval = selected_model.get_prediction(new_candy).conf_int()\nprint(f'Confidence Interval: {confidence_interval}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfidence Interval: [[53.65186346 70.48518939]]\n```\n:::\n:::\n\n\n:::\n\n::: fragment\n-   Prediction interval around $\\hat{y}$ for new data (predicted score for an individual type of candy with the given characteristics ):\n\n::: {#17845051 .cell execution_count=14}\n``` {.python .cell-code}\nprediction_interval = selected_model.get_prediction(new_candy).summary_frame(alpha=0.05)\nprint(f'Prediction Interval: {prediction_interval}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrediction Interval:         mean   mean_se  mean_ci_lower  mean_ci_upper  obs_ci_lower  \\\n0  62.068526  4.227679      53.651863      70.485189     39.549428   \n\n   obs_ci_upper  \n0     84.587625  \n```\n:::\n:::\n\n\n:::\n\n# Model validation\n\n**(optional, supplementary material)**\n\n## Overfitting {.smaller}\n\n::: incremental\n-   The data we are using to construct our models come from a larger population.\n\n-   Ultimately we want our model to tell us how the population works, not just the sample we have.\n\n-   If the model we fit is too tailored to our sample, it might not perform as well with the remaining population.\n    This means the model is \"overfitting\" our data.\n\n-   We measure this using **model validation** techniques.\n\n-   Note: Overfitting is not a huge concern with linear models with low level interactions, however it can be with more complex and flexible models.\n    The following is just an example of model validation, even though we're using it in a scenario where the concern for overfitting is not high.\n:::\n\n## Model validation {.smaller}\n\n::: incremental\n-   One commonly used model validation technique is to partition your data into training and testing set\n\n-   That is, fit the model on the training data\n\n-   And test it on the testing data\n\n-   Evaluate model performance using $RMSE$, root-mean squared error\n:::\n\n::: fragment\n$$ RMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}} $$\n:::\n\n::: fragment\n::: question\nDo you think we should prefer low or high RMSE?\n:::\n:::\n\n## Random sampling and reproducibility {.smaller}\n\nGotta set a seed!\n\n::: {#c7236f0b .cell execution_count=15}\n``` {.python .cell-code}\nnp.random.seed(1234)\n```\n:::\n\n\n::: incremental\n-   Use different seeds from each other\n\n-   Need inspiration?\n    <https://www.random.org/>\n:::\n\n## Cross validation {.smaller}\n\nMore specifically, **k-fold cross validation**\n\n::: columns\n::: {.column width=\"50%\"}\n::: incremental\n-   Split your data into k folds.\n\n-   Use 1 fold for testing and the remaining (k - 1) folds for training.\n\n-   Repeat k times.\n:::\n:::\n\n::: {.column width=\"50%\"}\n![](images/kfold.webp)\n:::\n:::\n\n## Prepping your data for 5-fold CV {.smaller}\n\n::: {#a28f41ce .cell execution_count=16}\n``` {.python .cell-code}\ncandy_rankings['id'] = np.arange(len(candy_rankings))\ncandy_rankings = candy_rankings.sample(frac=1).reset_index(drop=True)\ncandy_rankings['fold'] = (np.arange(len(candy_rankings)) % 5) + 1\n\ncandy_rankings_cv = candy_rankings.groupby('fold').size().reset_index(name='count')\nprint(candy_rankings_cv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   fold  count\n0     1     17\n1     2     17\n2     3     17\n3     4     17\n4     5     17\n```\n:::\n:::\n\n\n## CV 1 {.smaller}\n\n::: {#456a293c .cell execution_count=17}\n``` {.python .cell-code}\ntest_fold = 1\ntest = candy_rankings[candy_rankings['fold'] == test_fold]\ntrain = candy_rankings[candy_rankings['fold'] != test_fold]\n\nmodel = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=train).fit()\nrmse_test1 = np.sqrt(mean_squared_error(test['winpercent'], model.predict(test)))\nprint(f'RMSE Test 1: {rmse_test1}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE Test 1: 10.150090060666074\n```\n:::\n:::\n\n\n## RMSE on training vs. testing {.smaller}\n\n::: question\nWould you expect the RMSE to be higher for your training data or your testing data?\nWhy?\n:::\n\n## RMSE on training vs. testing {.smaller}\n\nRMSE for testing:\n\n::: {#ba8935b1 .cell execution_count=18}\n``` {.python .cell-code}\nrmse_test1 = np.sqrt(mean_squared_error(test['winpercent'], model.predict(test)))\nprint(f'RMSE Test 1: {rmse_test1}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE Test 1: 10.150090060666074\n```\n:::\n:::\n\n\nRMSE for training:\n\n::: {#cdf6fc65 .cell execution_count=19}\n``` {.python .cell-code}\nrmse_train1 = np.sqrt(mean_squared_error(train['winpercent'], model.predict(train)))\nprint(f'RMSE Train 1: {rmse_train1}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE Train 1: 10.117840760774035\n```\n:::\n:::\n\n\n## CV 2 {.smaller}\n\n::: {#abfed94f .cell execution_count=20}\n``` {.python .cell-code}\ntest_fold = 2\ntest = candy_rankings[candy_rankings['fold'] == test_fold]\ntrain = candy_rankings[candy_rankings['fold'] != test_fold]\nmodel = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=train).fit()\n```\n:::\n\n\n::: {#0a2c7cd3 .cell execution_count=21}\n``` {.python .cell-code}\nrmse_test2 = np.sqrt(mean_squared_error(test['winpercent'], model.predict(test)))\nrmse_train2 = np.sqrt(mean_squared_error(train['winpercent'], model.predict(train)))\nprint(f'RMSE Test 2: {rmse_test2}')\nprint(f'RMSE Train 2: {rmse_train2}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE Test 2: 10.432240639004073\nRMSE Train 2: 10.027804484574574\n```\n:::\n:::\n\n\n## CV 3 {.smaller}\n\n::: {#7f183e66 .cell execution_count=22}\n``` {.python .cell-code}\ntest_fold = 3\ntest = candy_rankings[candy_rankings['fold'] == test_fold]\ntrain = candy_rankings[candy_rankings['fold'] != test_fold]\nmodel = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=train).fit()\n```\n:::\n\n\n::: {#9deb8b9f .cell execution_count=23}\n``` {.python .cell-code}\nrmse_test3 = np.sqrt(mean_squared_error(test['winpercent'], model.predict(test)))\nrmse_train3 = np.sqrt(mean_squared_error(train['winpercent'], model.predict(train)))\nprint(f'RMSE Test 3: {rmse_test3}')\nprint(f'RMSE Train 3: {rmse_train3}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE Test 3: 11.958503120070846\nRMSE Train 3: 9.801042089669556\n```\n:::\n:::\n\n\n## CV 4 {.smaller}\n\n::: {#5278e6ac .cell execution_count=24}\n``` {.python .cell-code}\ntest_fold = 4\ntest = candy_rankings[candy_rankings['fold'] == test_fold]\ntrain = candy_rankings[candy_rankings['fold'] != test_fold]\nmodel = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=train).fit()\n```\n:::\n\n\n::: {#99d5048f .cell execution_count=25}\n``` {.python .cell-code}\nrmse_test4 = np.sqrt(mean_squared_error(test['winpercent'], model.predict(test)))\nrmse_train4 = np.sqrt(mean_squared_error(train['winpercent'], model.predict(train)))\nprint(f'RMSE Test 4: {rmse_test4}')\nprint(f'RMSE Train 4: {rmse_train4}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE Test 4: 12.399658584874498\nRMSE Train 4: 9.60646325325732\n```\n:::\n:::\n\n\n## CV 5 {.smaller}\n\n::: {#6bb3f4c0 .cell execution_count=26}\n``` {.python .cell-code}\ntest_fold = 5\ntest = candy_rankings[candy_rankings['fold'] == test_fold]\ntrain = candy_rankings[candy_rankings['fold'] != test_fold]\nmodel = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=train).fit()\n```\n:::\n\n\n::: {#28d8bea6 .cell execution_count=27}\n``` {.python .cell-code}\nrmse_test5 = np.sqrt(mean_squared_error(test['winpercent'], model.predict(test)))\nrmse_train5 = np.sqrt(mean_squared_error(train['winpercent'], model.predict(train)))\nprint(f'RMSE Test 5: {rmse_test5}')\nprint(f'RMSE Train 5: {rmse_train5}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE Test 5: 10.012905093096016\nRMSE Train 5: 10.13805597882432\n```\n:::\n:::\n\n\n## Putting it altogether {.smaller}\n\n::: {#26af5e89 .cell execution_count=28}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Dataframe\"}\nrmse_candy = pd.DataFrame({\n    'test_fold': np.arange(1, 6),\n    'rmse_train': [rmse_train1, rmse_train2, rmse_train3, rmse_train4, rmse_train5],\n    'rmse_test': [rmse_test1, rmse_test2, rmse_test3, rmse_test4, rmse_test5]\n})\n```\n:::\n\n\n::: {#3aba6b07 .cell execution_count=29}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Visual\"}\nplt.figure(figsize=(8, 5))\nsns.lineplot(data=rmse_candy, x='test_fold', y='rmse_test', marker='o', label='Test RMSE')\nplt.xlabel('Fold')\nplt.ylabel('RMSE')\nplt.title('Test RMSE for each fold')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](18-prediction-validation_files/figure-revealjs/cell-30-output-1.png){width=689 height=463}\n:::\n:::\n\n\n## How does RMSE compare to y? {.smaller}\n\n::: fragment\n-   `winpercent` summary stats:\n\n::: {#dc1a6ce2 .cell execution_count=30}\n\n::: {.cell-output .cell-output-stdout}\n```\ncount    85.000000\nmean     50.316764\nstd      14.714357\nmin      22.445341\n25%      39.141056\n50%      47.829754\n75%      59.863998\nmax      84.180290\nName: winpercent, dtype: float64\n```\n:::\n:::\n\n\n:::\n\n::: fragment\n-   `rmse_test` summary stats:\n\n::: {#c4bb590a .cell execution_count=31}\n\n::: {.cell-output .cell-output-stdout}\n```\ncount     5.000000\nmean     10.990679\nstd       1.106390\nmin      10.012905\n25%      10.150090\n50%      10.432241\n75%      11.958503\nmax      12.399659\nName: rmse_test, dtype: float64\n```\n:::\n:::\n\n\n:::\n\n## `model_selection` in scikit-learn {.smaller}\n\n::: columns\n::: {.column width=\"40%\"}\n![](images/scikit-learn.png)\n:::\n\n::: {.column width=\"60%\"}\nThe **scikit-learn** package provides functions that help you create pipelines when modeling.\n\n::: {#a1e4e765 .cell execution_count=32}\n``` {.python .cell-code}\nfrom sklearn.model_selection import KFold\n```\n:::\n\n\n:::\n:::\n\n::: aside\n[model selection via scikit-learn](https://scikit-learn.org/stable/model_selection.html)\n:::\n\n## Cross Validation - Faster {.smaller}\n\n-   **`sklearn.model_selection.KFold`**: Partition data into k folds\n\n-   Calculate RMSEs for each of the models on the testing set\n\n## Partition data into k folds {.smaller}\n\nk = 5:\n\n::: {#0f7e9b87 .cell execution_count=33}\n``` {.python .cell-code}\nkf = KFold(n_splits=5, shuffle=True, random_state=102319)\nfolds = list(kf.split(candy_rankings))\n```\n:::\n\n\n## Fit model on each of training set {.smaller}\n\n::: {#350712ba .cell execution_count=34}\n``` {.python .cell-code}\nrmses = []\nfor train_index, test_index in folds:\n    train_data = candy_rankings.iloc[train_index]\n    test_data = candy_rankings.iloc[test_index]\n    model = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=train_data).fit()\n    rmse = np.sqrt(mean_squared_error(test_data['winpercent'], model.predict(test_data)))\n    rmses.append(rmse)\n```\n:::\n\n\n## Calculate RMSEs {.smaller}\n\n::: columns\n::: {.column width=\"60%\"}\n\n::: {#7419ef39 .cell execution_count=35}\n``` {.python .cell-code code-fold=\"true\"}\nfold_ids = list(range(1, 6))\nplt.figure(figsize=(8, 5))\nplt.plot(fold_ids, rmses, marker='o', label='Test RMSE')\nplt.xlabel('Fold')\nplt.ylabel('RMSE')\nplt.title('Test RMSE for each fold')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](18-prediction-validation_files/figure-revealjs/cell-36-output-1.png){width=673 height=463}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\n<br> <br>\n\n::: {#dd001e77 .cell execution_count=36}\n``` {.python .cell-code}\nrmse_summary = pd.Series(rmses).describe()\nprint(rmse_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncount     5.000000\nmean     10.644100\nstd       3.751177\nmin       5.336119\n25%       8.319019\n50%      11.885830\n75%      13.232239\nmax      14.447295\ndtype: float64\n```\n:::\n:::\n\n\n:::\n:::\n\n",
    "supporting": [
      "18-prediction-validation_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}