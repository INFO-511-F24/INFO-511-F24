{
  "hash": "f8b73eac7cea95d0c720638d054f115b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 4 - Getting data into R\"\ncategories: \"Lab\"\n---\n\n\n# Introduction\n\nIn this lab you'll build the data wrangling and visualization skills you've developed so far and data tidying and joining to your repertoire.\n\n::: callout-note\nThis lab assumes you've completed the labs so far and doesn't repeat setup and overview content from those labs.\nIf you have not yet done those, you should go back and review the previous labs before starting on this one.\n:::\n\n## Learning objectives\n\nBy the end of the lab, you will...\n\n-   ...\n\n## Getting started\n\nLog in to RStudio, clone your `lab-3` repo from GitHub, open your `lab-3.qmd` document, and get started!\n\n::: {.callout-tip collapse=\"true\"}\n## Click here if you prefer to see step-by-step instructions\n\n### Log in to RStudio\n\n-   Go to <https://cmgr.oit.duke.edu/containers> and log in with your Duke NetID and Password.\n-   Click `STA198-199` under My reservations to log into your container. You should now see the RStudio environment.\n\n### Clone the repo & start new RStudio project\n\n-   Go to the course organization at [github.com/sta199-s24](https://github.com/sta199-s24) organization on GitHub.\n    Click on the repo with the prefix **lab-3**.\n    It contains the starter documents you need to complete the lab.\n\n-   Click on the green **CODE** button, select **Use SSH** (this might already be selected by default, and if it is, you'll see the text **Clone with SSH**).\n    Click on the clipboard icon to copy the repo URL.\n\n-   In RStudio, go to *File* ➛ *New Project* ➛*Version Control* ➛ *Git*.\n\n-   Copy and paste the URL of your assignment repo into the dialog box *Repository URL*. Again, please make sure to have *SSH* highlighted under *Clone* when you copy the address.\n\n-   Click *Create Project*, and the files from your GitHub repo will be displayed in the *Files* pane in RStudio.\n\n-   Click *lab-3.qmd* to open the template Quarto file.\n    This is where you will write up your code and narrative for the lab.\n\n### First steps\n\nIn `lab-3.qmd`, update the `author` field to your name, render your document and examine the changes.\nThen, in the Git pane, click on **Diff** to view your changes, add a commit message (e.g., \"Added author name\"), and click **Commit**.\nThen, push the changes to your GitHub repository, and in your browser confirm that these changes have indeed propagated to your repository.\n:::\n\n::: callout-important\nIf you run into any issues with the first steps outlined above, flag a TA for help before proceeding.\n:::\n\n## Packages\n\nIn this lab we will work with the **tidyverse** package, which is a collection of packages for doing data analysis in a \"tidy\" way.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n**Render** the document which loads this package with the `library()` function.\n\n## Guidelines\n\nAs we've discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\nIn addition, the code should all the code should be be able to be read (not run off the page) when you render to PDF.\nMake sure that is the case, and add line breaks where the code is running off the page.[^1]\n\n[^1]: Remember, haikus not novellas when writing code!\n\n::: callout-note\nContinuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course.\nThere will be periodic reminders in this assignment to remind you to **render, commit, and push** your changes to GitHub.\nYou should have at least 3 commits with meaningful commit messages by the end of the assignment.\n:::\n\n# Questions\n\n## Part 1 - Getting data in from CSVs\n\n## Part 2 - Getting data in from Excel files\n\n## Part 3 - Scraping data in from web pages\n\n### Question X\n\nPlease justify, using the tools we've learning in this course, if you are allowed to scrape data from each of the following websites:\n\n-   [www.espn.com](https://www.espn.com/)\n-   [twitter.com](https://twitter.com/)\n-   [www.rottentomatoes.com](https://www.rottentomatoes.com)\n\n### Question X\n\nRotten Tomatoes is an American review aggregation website for film.\nThey give percentage scores for movies based on how \"good\" the movies are.\nThey provide 2 scores:\n\n|                              |                                                                                                                   |\n|------------------------------|-------------------------------------------------------------------------------------------------------------------|\n| ![](images/popcornlogos.png) | The audience score, denoted by a popcorn bucket                                                                   |\n| ![](images/tomatologos.png)  | The Tomatometer score represents the percentage of professional critic reviews that are positive for a given film |\n\nWe are going to investigate the relationship between the audience score and Tomatometer score between the Halloween movies.\nPlease visit the following website to view the data we plan to scrape: <https://www.rottentomatoes.com/franchise/halloween>\n\na.  Using `Selector Gadget`, scrape the `tomato_score` and `audience_score` and report the lengths of these vectors (using the `length()`) function.\n    *Hint:* Their lengths should be equal.\n\nb.  Next, run the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitles_years <- page |>\n  html_nodes(\".franchise-media-list__h3\") |>\n  html_text2()\n  \nhalloween <- tibble(title_year = titles_years) |>\n  separate(title_year, into = c(\"title\", \"year\"), sep= \"\\\\(\" )\n```\n:::\n\n\nIn 2-3 sentences, describe what the above code is doing.\nMake sure to articulate each step of both of the pipelines.\n*Hint:* Print out `titles_years` and `halloween` to see what these objects look like.\nThis will help figure out what the code is doing.\nYou should also try running each line of the pipeline individually to see their outputs.\n\nc.  Add the columns `tomato_score` and `audience_score` from part (a) to your data frame called `halloween` that you created in part (b).\n\nd.  Create an appropriate plot the assess the relationship between a movie's audience score and their tomatometer score.\n    In 2-3 sentences, comment on the relationship.\n    Your plot should include appropriate labels.\n\n## Part 4 - Ethics\n\n### Question X\n\nOne current ethical discussion in data science involves the training of \"Large Language Models\" such as ChatGPT.\nThese models are trained using massive corpora (document sets) that include large amounts of work that is covered under copyright law.\nRead the following two articles:\n\n-   [Do Large Language Models Violate Copyright Law?](https://www.dykema.com/news-insights/do-large-language-models-violate-copyright-law.html)\n-   [Reexamining \"Fair Use\" in the Age of AI](https://hai.stanford.edu/news/reexamining-fair-use-age-ai)\n\nWrite a short paragraph (maximum 8 sentences) discussing the arguments on both sides of the discussion over copyright in training large language models.\n\n### Question X\n\nAnother major ethical discussion in data science resolves around discriminatory biases in machine learning models.\nThese biases can have real-world impacts in lending, criminal justice, hiring, and more.\nMany of these algorithms are so-called “black boxes”, meaning the exact process they take from input to output is unclear.\nRead the following articles:\n\n-   [Amazon scraps secret AI recruiting tool that showed bias against](https://www.ml.cmu.edu/news/news-archive/2016-2020/2018/october/amazon-scraps-secret-artificial-intelligence-recruiting-engine-that-showed-biases-against-women.html)\n-   [The Atlantic: The False Promise of Risk Assessment](https://www.theatlantic.com/technology/archive/2018/01/equivant-compas-algorithm/550646/)\n-   [Machine Bias: Risk Assessments in Criminal Sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\n\nWrite a short paragraph (maximum 8 sentences) discussing the nature of biases in machine learning and in datasets, and any possible solutions that could help limit those biases.\n\n# Wrap-up\n\n## Submission\n\nOnce you are finished with the lab, you will submit your final PDF document to Gradescope.\n\n::: callout-warning\nBefore you wrap up the assignment, make sure all of your documents are updated on your GitHub repo.\nWe will be checking these to make sure you have been practicing how to commit and push changes.\n\nYou must turn in a PDF file to the Gradescope page by the submission deadline to be considered \"on time\".\n:::\n\nTo submit your assignment:\n\n-   Go to <http://www.gradescope.com> and click *Log in* in the top right corner.\n-   Click *School Credentials* $\\rightarrow$ *Duke NetID* and log in using your NetID credentials.\n-   Click on your *STA 199* course.\n-   Click on the assignment, and you'll be prompted to submit it.\n-   Mark all the pages associated with question. All the pages of your lab should be associated with at least one question (i.e., should be \"checked\").\n\n::: callout-important\n## Checklist\n\nMake sure you have:\n\n-   attempted all questions\n-   rendered your Quarto document\n-   committed and pushed everything to your GitHub repository such that the Git pane in RStudio is empty\n-   uploaded your PDF to Gradescope\n-   selected pages associated with each question on Gradescope\n:::\n\n## Grading\n\nThe lab is graded out of a total of 50 points.\n\nYou can earn up to 5 points on each question:\n\n-   5: Response shows excellent understanding and addresses all or almost all of the rubric items.\n\n-   4: Response shows good understanding and addresses most of the rubric items.\n\n-   3: Response shows understanding and addresses a majority of the rubric items.\n\n-   2: Response shows effort and misses many of the rubric items.\n\n-   1: Response does not show sufficient effort or understanding and/or is largely incomplete.\n\n-   0: No attempt.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}